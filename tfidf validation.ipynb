{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct = 'odi'\n",
    "\n",
    "total = {'odi' : 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# params\n",
    "from utils import read_config\n",
    "\n",
    "config = read_config()\n",
    "\n",
    "DATAROOT = \"../data/\"\n",
    "PLOTROOT = \"../plots/\"\n",
    "\n",
    "pros_cons = ['pros', 'cons']\n",
    "\n",
    "# params\n",
    "sent_based = True\n",
    "text = config['TEXTS'][construct]\n",
    "num_goals = len(config['COUNTS'][construct])\n",
    "THRESHOLD = config['THRESHOLDS'][construct]\n",
    "PRESET = config['PRESETS'][construct]\n",
    "primary_goal_shorthand = config['SHORTHANDS'][construct]\n",
    "final_goals = config['FINAL'][construct]\n",
    "scoring = config['SCORING'][construct]\n",
    "reference_text = config['REFERENCES'][construct]\n",
    "filename = config['REFERENCE_DATAS'][construct]\n",
    "sim_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import subset_by_percentile, subset_by_percentile_or_preset, sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = pd.read_csv(DATAROOT + \"review_us_master.csv\")\n",
    "onek_data = sample_data(main_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = pd.read_csv(DATAROOT + filename, sep = \"\\t\")\n",
    "reference_name = 'item text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = main_data.dropna(subset = [text], axis = 0)\n",
    "#main_data['embedding'] = list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import find_similarity, plot_sim_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### do once and save\n",
    "# for i in range(num_goals):\n",
    "#     main_data['%d_sim_1' %(i)] = find_similarity(main_data['embedding'].values,\n",
    "#                                                sustainability['embedding_1'].values[i])\n",
    "#     main_data['%d_sim_2' %(i)] = find_similarity(main_data['embedding'].values,\n",
    "#                                                sustainability['embedding_2'].values[i])  \n",
    "    \n",
    "# # save similar data but drop the embeddings\n",
    "# main_data = main_data.drop('embedding', axis = 1)\n",
    "\n",
    "# main_data.to_csv(DATAROOT+\"intermediate/%s_embedded_sim_review.csv\" %(text), sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_datas = {}\n",
    "main_datas_ = {}\n",
    "for text in [text]:\n",
    "    if sent_based:\n",
    "        embed_file = \"intermediate/%s_%s_sent_embedded.csv\" %(construct, text)\n",
    "    else:\n",
    "        embed_file = \"intermediate/%s_embedded_sim_review.csv\" %(text) # original full pro or con\n",
    "        \n",
    "    main_datas_[text] = pd.read_csv(DATAROOT+embed_file, sep = \"\\t\")\n",
    "    main_datas[text] = []\n",
    "    main_datas[text].append(subset_by_percentile_or_preset(main_datas_[text],\n",
    "                                                           THRESHOLD, simfield = '_sim_1',\n",
    "                                                           preset = PRESET, num = total[construct]))\n",
    "    #main_datas[text].append(subset_by_percentile(main_datas_[text], 0.90, simfield = '_sim_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from string import digits\n",
    "import re\n",
    "\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "stops = stopwords.words('english')\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def find_keywords(main_datas, fulltext = True, textfield = \"whole_review_text\",\n",
    "                  n_gram_range = range(4), sim_num = 0, num_goals = num_goals):\n",
    "    # step 1: find keywords\n",
    "    goal_keywords = []\n",
    "    documents = []\n",
    "    if sent_based:\n",
    "        textfield_ = textfield + \"_sent\"\n",
    "\n",
    "    print(num_goals)\n",
    "    # basic preprocessing --- a) lower case, b) remove punctuaction\n",
    "    for num in range(num_goals):\n",
    "        dataset = main_datas[textfield][sim_num][num].copy()\n",
    "        dataset = dataset.dropna(subset = [textfield_], axis = 0)\n",
    "        #print(len(dataset))\n",
    "        doc = ' '.join(dataset[textfield].values).lower().translate(string.punctuation).translate(remove_digits)\n",
    "        doc = re.sub(r'[^\\w\\s]','',doc).replace('_', '')\n",
    "        split_doc = [lemma.lemmatize(i, pos=\"v\") for i in doc.split() if i not in stops]\n",
    "        documents.append(' '.join(split_doc)  )\n",
    "        \n",
    "        keywords = []    \n",
    "        if not fulltext:\n",
    "            for n in n_gram_range:\n",
    "                ngram_counts = Counter(ngrams(split_doc, n)).most_common(100)\n",
    "                keywords.append([\" \".join(list(i)) for (i,j) in ngram_counts])\n",
    "            goal_keywords.append(keywords)          \n",
    "    \n",
    "    if fulltext:\n",
    "        megadoc = [i.split(\" \") for i in documents]\n",
    "        megadoc = [item for sublist in megadoc for item in sublist]\n",
    "        for n in n_gram_range:\n",
    "            ngram_counts = Counter(ngrams(megadoc, n)).most_common(100)\n",
    "            #print(n, ngram_counts[:5])\n",
    "            keywords.append([\" \".join(list(i)) for (i,j) in ngram_counts])\n",
    "        goal_keywords.append(keywords)\n",
    "    return goal_keywords, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(documents, n_gram_range):\n",
    "    vect = TfidfVectorizer(ngram_range = n_gram_range)\n",
    "    tfidf_matrix = vect.fit_transform(documents)\n",
    "    tfidf_scores = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\n",
    "    features = vect.get_feature_names()\n",
    "    return vect, features, tfidf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_scores(keywords, features, tfidf_scores, fulltext = True, num_goals = num_goals):\n",
    "    \n",
    "    all_goal_keywords = keywords.copy()\n",
    "\n",
    "    if not fulltext:\n",
    "        tfidfs_per_goal = []\n",
    "        \n",
    "\n",
    "        for goal in range(num_goals):\n",
    "            all_goal_keywords[goal] = [item for sublist in all_goal_keywords[goal] for item in sublist]\n",
    "            #all_goal_keywords[goal] = [\" \".join(i) if type(i) == tuple else i for i in all_goal_keywords[goal]]\n",
    "            shortlist = list(set(all_goal_keywords[goal]).intersection(features))\n",
    "            tfidfs_per_goal.append(tfidf_scores[shortlist])\n",
    "        tfidfs = tfidfs_per_goal\n",
    "    else:\n",
    "        goal= 0\n",
    "        all_goal_keywords[goal] = [item for sublist in all_goal_keywords[goal] for item in sublist]\n",
    "        #all_goal_keywords[goal] = [\" \".join(i) if type(i) == tuple else i for i in all_goal_keywords[goal]]\n",
    "        #print(all_goal_keywords[goal])\n",
    "        shortlist = list(set(all_goal_keywords[goal]).intersection(features))\n",
    "        tfidfs = [tfidf_scores[shortlist]]\n",
    "        \n",
    "    return tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_order = [0, 1, 7, 8, 2, 3, 5, 6, 12, 4, 9, 10, 11, 13, 14, 15, 16]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "def get_tfidf_heatmap(tfidfs, goal = 0, title = \"all goals TF-IDF\",\n",
    "                      textfield = 'whole_review_text', \n",
    "                      mapping = {'whole_review_text' : 'Red'},\n",
    "                      ordered = False,\n",
    "                      reduced = False, \n",
    "                      new_reduced_order = final_goals,\n",
    "                      even_more_reduced = False,\n",
    "                      even_more_reduced_order = [2, 4],\n",
    "                      reference = reference,\n",
    "                      reference_name = \"Goal\"\n",
    "                     ):\n",
    "    #f, ax = plt.subplots(figsize=(30, 5))\n",
    "    \n",
    "    ordering = range(17)\n",
    "\n",
    "    df = scaler.fit_transform(tfidfs[goal])\n",
    "    df = df.sort_values(by=0, ascending=False, axis=1)\n",
    "    if ordered:\n",
    "        if reduced:\n",
    "            if even_more_reduced:\n",
    "                new_reduced_order = even_more_reduced_order\n",
    "            df = df.reindex(new_reduced_order)\n",
    "            ordering = new_reduced_order\n",
    "        else:\n",
    "            df = df.reindex(new_order)\n",
    "            ordering = new_order\n",
    "        title = title + ' ordered'\n",
    "    df = df.loc[:, (df < 0.02).any(axis=0)]\n",
    "    \n",
    "    \n",
    "    print(len(df.columns))\n",
    "    dfs = [df[df.columns[:30]]]\n",
    "    dfs.append(df[df.columns[30:60]])\n",
    "    dfs.append(df[df.columns[60:90]])\n",
    "    dfs.append(df[df.columns[90:120]])\n",
    "    dfs.append(df[df.columns[120:150]])\n",
    "    dfs.append(df[df.columns[150:180]])\n",
    "    dfs.append(df[df.columns[180:210]])\n",
    "    dfs.append(df[df.columns[210:240]])\n",
    "\n",
    "    #df = df != 0\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = 4, ncols = 2, figsize = (18, 15))\n",
    "    \n",
    "    num = 0\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 2):\n",
    "            sns.heatmap(dfs[num], cmap = mapping[textfield], linewidths = .5, ax = ax[i][j])\n",
    "            labels = list(reference[reference_name])\n",
    "    \n",
    "            labels_ = []\n",
    "            for n in ordering:\n",
    "                labels_.append(primary_goal_shorthand[labels[n]])\n",
    "\n",
    "            ax[i][j].set_yticklabels(labels_)\n",
    "            ax[i][j].yaxis.set_tick_params(rotation=0)\n",
    "            \n",
    "            num += 1\n",
    "            \n",
    "    #fig.suptitle(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(PLOTROOT + \"validation/tfidf_sent_based/%s.pdf\" %(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_goal_shorthand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_order = [0, 1, 7, 8, 2, 3, 5, 6, 12, 4, 9, 10, 11, 13, 14, 15, 16]\n",
    "\n",
    "\n",
    "\n",
    "def get_tfidf_heatmap(tfidfs, goal = 0, title = \"all goals TF-IDF\",\n",
    "                      textfield = 'whole_review_text', \n",
    "                      mapping = {'whole_review_text' : 'Red'},\n",
    "                      ordered = False,\n",
    "                      reduced = False, \n",
    "                      new_reduced_order = final_goals,\n",
    "                      even_more_reduced = False,\n",
    "                      even_more_reduced_order = [2, 4],\n",
    "                      reference = reference,\n",
    "                      reference_name = reference_text,\n",
    "                      shorthands = primary_goal_shorthand,\n",
    "                      num_goals = num_goals,\n",
    "                     ):\n",
    "    #f, ax = plt.subplots(figsize=(30, 5))\n",
    "    \n",
    "    ordering = range(num_goals)\n",
    "\n",
    "    df = tfidfs[goal]\n",
    "    df = df.sort_values(by=0, ascending=False, axis=1)\n",
    "#     if ordered:\n",
    "#         if reduced:\n",
    "#             if even_more_reduced:\n",
    "#                 new_reduced_order = even_more_reduced_order\n",
    "#             df = df.reindex(new_reduced_order)\n",
    "#             ordering = new_reduced_order\n",
    "#         else:\n",
    "#             df = df.reindex(new_order)\n",
    "#             ordering = new_order\n",
    "#         title = title + ' ordered'\n",
    "    df = df.loc[:, (df < 0.01).any(axis=0)]\n",
    "    #x_scaled = scaler.fit_transform(df)\n",
    "    #df = pd.DataFrame(x_scaled, columns = df.columns)\n",
    "    \n",
    "    \n",
    "    print(len(df.columns))\n",
    "    dfs = [df[df.columns[:50]]]\n",
    "    dfs.append(df[df.columns[50:100]])\n",
    "    dfs.append(df[df.columns[100:150]])\n",
    "\n",
    "    #df = df != 0\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = 3, ncols = 1, figsize = (18, 18))\n",
    "    \n",
    "    num = 0\n",
    "    print(ordering)\n",
    "    \n",
    "    for i in range(0, 3):\n",
    "        for j in range(0, 1):\n",
    "            #dfs[num] = dfs[num].sort_values(by=num, ascending=False, axis=1)\n",
    "            sns.heatmap(dfs[num], cmap = mapping[textfield], linewidths = .5, ax = ax[i])\n",
    "            labels = list(reference[reference_name])\n",
    "    \n",
    "            labels_ = []\n",
    "            for n in ordering:\n",
    "                labels_.append(shorthands[labels[n]])\n",
    "\n",
    "            ax[i].set_yticklabels(labels_, fontsize = 18)\n",
    "            ax[i].yaxis.set_tick_params(rotation=0)\n",
    "            ax[i].set_xticklabels(dfs[num].columns, fontsize = 16)\n",
    "            \n",
    "            num += 1\n",
    "            \n",
    "    #fig.suptitle(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(PLOTROOT + \"validation/tfidf_sent_based/%s.pdf\" %(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colormap = {'pros' : 'YlGn', 'cons' : 'OrRd', 'whole_review_text' : 'PuBu'}\n",
    "  \n",
    "main_datas = {}\n",
    "main_datas_ = {}\n",
    "for text in pros_cons:\n",
    "    embed_file = \"intermediate/%s_%s_sent_embedded.csv\" %(construct, text)\n",
    "    main_datas_[text] = pd.read_csv(DATAROOT+embed_file, sep = \"\\t\")\n",
    "    main_datas[text] = []\n",
    "    main_datas[text].append(subset_by_percentile_or_preset(main_datas_[text], THRESHOLD, simfield = '_sim_1',\n",
    "                                                           preset = PRESET, num = total[construct]))\n",
    "    #main_datas[text].append(subset_by_percentile(main_datas_[text], 0.90, simfield = '_sim_2'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1\n",
    "end = 4\n",
    "n_gram_range = range(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for textfield in [\"cons\"]: \n",
    "    per_goal_keywords, documents = find_keywords(main_datas, fulltext = False, textfield = textfield,\n",
    "                                                 n_gram_range = n_gram_range, sim_num = sim_num,\n",
    "                                                 num_goals = total[construct])  \n",
    "    keywords, documents = find_keywords(main_datas, textfield = textfield, n_gram_range = n_gram_range,\n",
    "                                        sim_num = sim_num, num_goals = total[construct])  \n",
    "    vect, features, tfidf_scores = get_tfidf(documents, n_gram_range = (start,end-1))\n",
    "    per_goal_tfidfs = get_tfidf_scores(per_goal_keywords, features, tfidf_scores, \n",
    "                                       fulltext = False, num_goals = total[construct])  \n",
    "    tfidfs = get_tfidf_scores(keywords, features, tfidf_scores)    \n",
    "    title = \"all goals (30 keywords) TF-IDF on %s (Def = %d, \\\n",
    "    Threshold = %0.2f, preset_sim = %0.2f)\" %(textfield, sim_num, THRESHOLD, PRESET)    \n",
    "    title = title + \" %d-%d\" %(n_gram_range[0], n_gram_range[-1])    \n",
    "    #get_tfidf_heatmap(tfidfs_, 0, title = title, textfield = textfield, mapping = colormap, ordered = True)  \n",
    "    get_tfidf_heatmap(tfidfs, 0, title = title, textfield = textfield, mapping = colormap, \n",
    "                      ordered = True, reduced = True, reference = reference, reference_name = reference_name,\n",
    "                      shorthands = primary_goal_shorthand, num_goals = total[construct]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tfidfs[0]\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "pd.DataFrame(x_scaled, columns = tfidfs[0].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_handpicked_heatmap(tfidfs, keywords, goal = 0, title = \"all goals TF-IDF\",\n",
    "                      textfield = 'whole_review_text', \n",
    "                      mapping = {'whole_review_text' : 'Red'},\n",
    "                      ordered = False,\n",
    "                      reduced = False, \n",
    "                      new_reduced_order = final_goals,\n",
    "                      even_more_reduced = False,\n",
    "                      even_more_reduced_order = [2, 4],\n",
    "                      reference = reference,\n",
    "                      reference_name = \"Goal\"\n",
    "                     ):\n",
    "    #f, ax = plt.subplots(figsize=(30, 5))\n",
    "    \n",
    "    ordering = range(num_goals)\n",
    "\n",
    "    df = tfidfs[goal]\n",
    "    df = df.sort_values(by=0, ascending=False, axis=1)\n",
    "    if ordered:\n",
    "        if reduced:\n",
    "            if even_more_reduced:\n",
    "                new_reduced_order = even_more_reduced_order\n",
    "            df = df.reindex(new_reduced_order)\n",
    "            ordering = new_reduced_order\n",
    "        else:\n",
    "            df = df.reindex(new_order)\n",
    "            ordering = new_order\n",
    "        title = title + ' ordered'\n",
    "    df = df.loc[:, (df < 0.02).any(axis=0)]\n",
    "    #x_scaled = scaler.fit_transform(df)\n",
    "    #df = pd.DataFrame(x_scaled, columns = df.columns)\n",
    "    \n",
    "    \n",
    "    print(len(df.columns))\n",
    "    dfs = [df[df.columns[:50]]]\n",
    "    dfs.append(df[df.columns[50:100]])\n",
    "    dfs.append(df[df.columns[100:150]])\n",
    "\n",
    "    #df = df != 0\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = 3, ncols = 1, figsize = (18, 18))\n",
    "    \n",
    "    num = 0\n",
    "    for i in range(0, 3):\n",
    "        for j in range(0, 1):\n",
    "            #dfs[num] = dfs[num].sort_values(by=num, ascending=False, axis=1)\n",
    "            sns.heatmap(dfs[num], cmap = mapping[textfield], linewidths = .5, ax = ax[i])\n",
    "            labels = list(reference[reference_name])\n",
    "    \n",
    "            labels_ = []\n",
    "            for n in ordering:\n",
    "                labels_.append(primary_goal_shorthand[labels[n]])\n",
    "\n",
    "            ax[i].set_yticklabels(labels_, fontsize = 18)\n",
    "            ax[i].yaxis.set_tick_params(rotation=0)\n",
    "            ax[i].set_xticklabels(dfs[num].columns, fontsize = 16)\n",
    "            \n",
    "            num += 1\n",
    "            \n",
    "    #fig.suptitle(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(PLOTROOT + \"validation/tfidf_sent_based/%s.pdf\" %(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_keywords = [\n",
    "  'work', 'great', 'good', 'benefit', 'balance', 'environment', 'people', 'life', 'opportunities', 'pay', 'culture', 'company', 'learn', 'growth', 'lot', 'employees', 'train', 'worklife', 'career', 'job', 'management', 'opportunity', 'time', 'team', 'excellent', 'friendly', 'flexible', 'get', 'development', 'place', 'grow', 'employee', 'hours', 'salary', 'help', 'coworkers', 'advancement', 'care', 'decent', 'support', 'make', 'nice', 'many', 'health', 'leadership', 'experience', 'well', 'diversity', 'compensation', 'new', 'strong', 'provide', 'value', 'schedule', 'flexibility', 'best', 'program', 'amaze', 'really', 'project', 'competitive', 'encourage', 'supportive', 'smart', 'k', 'also', 'take', 'business', 'fun', 'focus', 'home', 'professional', 'community', 'family', 'always', 'within', 'like', 'challenge', 'free', 'give', 'different', 'build', 'skills', 'offer', 'resources', 'technology', 'industry', 'managers', 'everyone', 'perk', 'interest', 'hard', 'atmosphere', 'one', 'level', 'easy', 'positive', 'promote', 'want', 'room'], ['work life', 'life balance', 'great benefit', 'good work', 'worklife balance', 'work environment', 'good benefit', 'benefit good', 'great work', 'balance good', 'benefit great', 'great people', 'good pay', 'balance great', 'pay benefit', 'environment good', 'work culture', 'environment great', 'people work', 'great culture', 'good people', 'benefit work', 'place work', 'great pay', 'good worklife', 'flexible work', 'people great', 'pay good', 'great place', 'work great', 'work home', 'great company', 'people good', 'work good', 'good environment', 'balance work', 'good culture', 'culture good', 'pay great', 'growth opportunities', 'smart people', 'great environment', 'culture great', 'career growth', 'environment work', 'excellent benefit', 'friendly environment', 'job security', 'learn opportunities', 'company work', 'great worklife', 'decent pay', 'benefit pay', 'health benefit', 'lot opportunities', 'flexible schedule', 'opportunities great', 'care employees', 'work hours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotated_heatmap(tfidfs, goal = 0, title = \"all goals TF-IDF\",\n",
    "                      textfield = 'whole_review_text', \n",
    "                      mapping = {'whole_review_text' : 'Red'},\n",
    "                      ordered = False,\n",
    "                      reduced = False, \n",
    "                      new_reduced_order = final_goals,\n",
    "                      even_more_reduced = False,\n",
    "                      even_more_reduced_order = [2, 4],\n",
    "                      chosen_keywords = None,\n",
    "                      reference = reference,\n",
    "                      reference_name = reference_text\n",
    "                     ):\n",
    "    #f, ax = plt.subplots(figsize=(30, 5))\n",
    "    \n",
    "    ordering = range(num_goals)\n",
    "\n",
    "    df = tfidfs[goal]\n",
    "    if chosen_keywords:\n",
    "        df = df[chosen_keywords]\n",
    "    df = df.sort_values(by=0, ascending=False, axis=1)\n",
    "    if ordered:\n",
    "        if reduced:\n",
    "            if even_more_reduced:\n",
    "                new_reduced_order = even_more_reduced_order\n",
    "            df = df.reindex(new_reduced_order)\n",
    "            ordering = new_reduced_order\n",
    "        else:\n",
    "            df = df.reindex(new_order)\n",
    "            ordering = new_order\n",
    "        title = title + ' ordered'\n",
    "    df = df.loc[:, (df < 0.02).any(axis=0)]\n",
    "    x_scaled = scaler.fit_transform(df)\n",
    "    df = pd.DataFrame(x_scaled, columns = df.columns)\n",
    "    \n",
    "    \n",
    "    print(len(df.columns))\n",
    "    #print(list(df.columns))\n",
    "#     print(keywords)\n",
    "#     df = df[chosen_keywords]\n",
    "#     print(df)\n",
    "    dfs = [df[df.columns[:25]]]\n",
    "    dfs.append(df[df.columns[30:60]])\n",
    "    dfs.append(df[df.columns[60:90]])\n",
    "    \n",
    "\n",
    "    #df = df != 0\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (16, 5))\n",
    "    \n",
    "    num = 0\n",
    "    for i in range(0, 1):\n",
    "        for j in range(0, 1):\n",
    "            #dfs[num] = dfs[num].sort_values(by=num, ascending=False, axis=1)\n",
    "            sns.heatmap(dfs[num], cmap = mapping[textfield], linewidths = .5, ax = ax)\n",
    "            labels = list(reference[reference_name])\n",
    "    \n",
    "            labels_ = []\n",
    "            for n in ordering:\n",
    "                labels_.append(primary_goal_shorthand[labels[n]])\n",
    "\n",
    "            ax.set_yticklabels(labels_, fontsize = 18)\n",
    "            ax.yaxis.set_tick_params(rotation=0)\n",
    "            ax.set_xticklabels(dfs[num].columns, fontsize = 14)\n",
    "            \n",
    "            bottom, top = ax.get_ylim()\n",
    "            ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "            \n",
    "            num += 1\n",
    "            \n",
    "    #fig.suptitle(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTROOT + 'tfidf.pdf')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36] *",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
